ai_test_foundation ‚Äî Optimization Phase

Module: SmartLocator + AIHealer Optimization
Owner: Ram
Mentor: Yogi (via ChatGPT)
Repository: ai_test_foundation

üéØ Objective

Optimize the SmartLocator and AIHealer components for:

Performance (faster healing, fewer redundant AI calls)

Reliability (error resilience, clean fallback mechanisms)

Efficiency (cache reuse, batch prompt handling, logging improvements)

Maintainability (cleaner method-level structure and comments)

You will not change architecture, but refactor & harden both modules to enterprise-grade stability.

üß† Background Context

The current SmartLocator and AIHealer work perfectly for self-healing DOM-based locator repair.
However, they call the AI provider repeatedly for similar repair requests, and some edge cases (timeouts, invalid responses, rate limits) need more robust handling.

Both components live under:

core/
‚îú‚îÄ‚îÄ smart_locator/
‚îÇ   ‚îú‚îÄ‚îÄ smart_locator.py
‚îÇ   ‚îú‚îÄ‚îÄ smart_page.py
‚îÇ   ‚îî‚îÄ‚îÄ framework_adapter.py
‚îî‚îÄ‚îÄ ai_healer.py


The AI interaction is routed through:

services/locator_repair/repair_service.py
services/locator_repair/ai_gateway.py

üß∞ Work to Do
1Ô∏è‚É£ Add AI-Healing Cache Layer

Create or extend a local cache in ai_healer.py:

Store previously healed locator pairs:
(failed_locator + context_hint) ‚Üí healed_locator

Save to logs/healing_cache.json

On subsequent failures:

Check cache first

If found, skip API call

If not found, call AI provider and cache result

# Example snippet
cache_key = f"{framework}:{failed_locator}:{context_hint}"
if cache_key in self.cache:
    return self.cache[cache_key]
else:
    healed = self.call_ai(...)
    self.cache[cache_key] = healed
    self.save_cache()

2Ô∏è‚É£ Add Retry + Backoff Logic for AI Calls

Wrap all AI calls with a retry loop (max 3 attempts).

Use exponential backoff (1s ‚Üí 2s ‚Üí 4s).

Handle errors gracefully (Timeout, ConnectionError, or malformed AI response).

Log retries cleanly.

for attempt in range(3):
    try:
        healed = ai_gateway.ask(prompt)
        break
    except Exception as e:
        wait = 2 ** attempt
        time.sleep(wait)
        if attempt == 2:
            log.error(f"AI healing failed after 3 attempts: {e}")
            return failed_locator

3Ô∏è‚É£ Improve AI Response Sanitization

Some AI responses still include markdown or JSON.

Add a sanitizer utility in ai_healer.py:

Remove backticks, quotes, markdown, and explanations.

Extract the first valid locator string from text or JSON.

def clean_ai_response(resp: str) -> str:
    if resp.startswith("```"): resp = resp.strip("`")
    resp = resp.strip('`"\' ')
    if "locator" in resp.lower():
        resp = resp.split(":")[-1].strip()
    return resp.split("\n")[0]

4Ô∏è‚É£ Enhance SmartLocator Logging

Improve logs in logs/healing_log.json to include:

healing_source: "cache" | "ai" | "fallback"

latency_ms: total repair time

confidence: optional field from AI if provided

Use Python time.perf_counter() to track elapsed healing time.

5Ô∏è‚É£ Add Internal Fallback Hierarchy

If AI healing fails completely:

Try context_hint-based heuristic:

Example: "Submit" ‚Üí find button with similar text via framework adapter.

Log fallback usage.

Mark it as "healing_source": "fallback" in logs.

6Ô∏è‚É£ Clean Method Docstrings

Ensure every method in smart_locator.py and ai_healer.py has:

Type hints

One-line summary

Usage example in docstring

Example:

def heal_locator(self, page, failed_locator: str, context_hint: str = "") -> str:
    """
    Attempt to repair a failed locator using AI or cached results.
    
    Args:
        page (Any): Framework page/driver instance
        failed_locator (str): The broken locator string
        context_hint (str): Optional context for AI healing
        
    Returns:
        str: Repaired locator string
    """

7Ô∏è‚É£ Unit Tests for Optimization

Create a new test file:

tests/test_ai_healer_optimization.py


Test cases:

‚úÖ Healing cache hit (no API call)

‚úÖ Healing cache miss (API called)

‚úÖ Retry logic (simulate failure, then success)

‚úÖ Response cleaning (remove markdown)

‚úÖ Fallback recovery

‚úÖ Log structure validation (fields present)

üì¶ Deliverables

Updated core/ai_healer.py

Updated core/smart_locator/smart_locator.py

New file tests/test_ai_healer_optimization.py

Updated logs/healing_log.json and new logs/healing_cache.json

Updated docstrings and inline comments
=====================================================================
Next Action Steps

1Ô∏è‚É£ Integrate new AI-Healer

Replace the existing ai_healer.py in your repo with Ram‚Äôs optimized version.

Ensure the caching and logging directories (/cache, /logs) exist.

2Ô∏è‚É£ Run validation tests

pytest -v tests/test_ai_healer_optimization.py


confirm 100 % pass rate.

3Ô∏è‚É£ Wire back into SmartLocator
update import:

from core.ai_healer import AIHealer


and confirm caching path consistency.

4Ô∏è‚É£ Push to Git
commit as:

feat(ai_healer): integrate enterprise-grade optimized AIHealer from Ram


5Ô∏è‚É£ Proceed to Vision Testing module
once the healer is verified, we move to Phase 4 ‚Äì Vision LLM visual validation.
=================================================================
Reality Check ‚Äî What Might Be Missing
Area	What He Claimed	What You Must Verify
AI Cache Layer	He didn‚Äôt mention caching at all.	Check if logs/healing_cache.json was created and loaded in ai_healer.py.
Retry & Backoff	Not mentioned.	Search for for attempt in range( or time.sleep ‚Äî if missing, retry logic wasn‚Äôt added.
Response Sanitization	Likely included implicitly.	Confirm function clean_ai_response() exists and strips markdown.
SmartLocator Logging Enhancements	Not mentioned explicitly.	Verify new fields in logs/healing_log.json: healing_source, latency_ms, confidence.
Fallback Recovery	No mention.	Check if there‚Äôs a heuristic fallback (e.g., context-based search in adapters).
Unit Test Coverage	‚Äú25/25 tests passed‚Äù sounds like he reran the whole suite, not just the new ones.	Ensure tests/test_ai_healer_optimization.py exists and explicitly tests cache, retry, fallback.
Commit Message	He said ‚Äúas Jack requested.‚Äù	You need to check the actual Git commit log to ensure changes are isolated, not mixed.
üß† Next Steps for You (Verification Path)
1Ô∏è‚É£ Ask Ram for Exact Commit Hash

Run:

git log -1 --oneline


You should get something like:

a9b7c32 Refactor ai_healer.py with caching and retry logic


Then confirm what files changed:

git show --stat a9b7c32


Expect to see:

core/ai_healer.py | +200 -50
tests/test_ai_healer_optimization.py | +120
2 files changed, 320 insertions(+), 50 deletions(-)


If he only touched ai_healer.py, caching or logging updates might still be missing.
================================================================
Run Targeted Test

Check specifically for optimization coverage:

pytest -s -v tests/test_ai_healer_optimization.py


If this file doesn‚Äôt exist, Ram skipped the new tests ‚Äî that‚Äôs an immediate gap.

3Ô∏è‚É£ Inspect Log Output

After running healing once, open logs/healing_log.json and confirm:

{
  "timestamp": "...",
  "old_locator": "button#wrong",
  "new_locator": "button#submit",
  "healing_source": "cache",
  "latency_ms": 120,
  "confidence": 0.93
}


If those fields are missing ‚Üí optimization incomplete.

4Ô∏è‚É£ Check Cache Behavior

Run the same test twice:

pytest -s -v tests/test_ai_healing_dual.py


Run 1: Should show ‚Äúüîß Attempting AI-powered repair‚Ä¶‚Äù

Run 2: Should show ‚Äú‚ö° Using cached repair result‚Ä¶‚Äù (no API call)

If every test triggers the AI again, caching isn‚Äôt implemented.

5Ô∏è‚É£ Confirm Retry Logic

Temporarily simulate failure by editing .env with a wrong API key, then rerun:

pytest -s -v tests/test_ai_healing.py


You should see something like:

‚ö†Ô∏è AI request failed, retrying in 1s...
‚ö†Ô∏è AI request failed, retrying in 2s...
‚ùå AI healing failed after 3 attempts.


If it just fails immediately ‚Üí retry logic missing.
=================================================================
ai_test_foundation ‚Äî Phase 4: Vision LLM Testing Module

Module: Vision Analyzer + Visual Validation
Owner: Ram
Mentor: Yogi (via ChatGPT)
Repository: ai_test_foundation

üéØ Objective

Add AI-powered visual comparison and visual healing so tests can detect and fix UI differences that pure DOM healing misses.

The goal:
Compare screenshots ‚Üí analyze via Vision LLM ‚Üí detect UI anomalies ‚Üí report & optionally auto-heal.

üß† Background Context

Core AI-Healer (optimized) already handles DOM-based locator repair.

Phase 4 extends it with a VisionAnalyzer that uses multimodal LLM (e.g., Gemini Vision or GPT-4V) to perform visual diff analysis.

Vision results feed into AI-Healer as a secondary healing signal (e.g., ‚ÄúButton moved left 20px‚Äù).

üß∞ Work to Do
1Ô∏è‚É£ Create core/vision_analyzer.py

Implement class VisionAnalyzer with methods:

class VisionAnalyzer:
    def __init__(self, provider: str = "gemini", cache_dir="logs/vision_cache"):
        """Initialize Vision Analyzer with provider (gemini | openai)."""

    def compare_images(self, baseline_path: str, current_path: str) -> dict:
        """Return visual diff data (regions, labels, confidence)."""

    def detect_visual_anomalies(self, baseline_path: str, current_path: str, threshold: float = 0.8) -> list:
        """Return list of significant visual changes above threshold."""

    def analyze_with_llm(self, diff_image_path: str, prompt: str = "") -> dict:
        """Ask Vision LLM to describe or explain visual differences."""


‚úÖ Features to include

Local image comparison (PIL + NumPy for diff map)

Base64 encoding ‚Üí send to LLM via ai_gateway.py

Cache visual analysis results (logs/vision_cache.json)

Return structured JSON: {region, description, confidence}

2Ô∏è‚É£ Extend ai_gateway.py

Add multimodal support:

def ask_vision(self, image_paths: list[str], question: str) -> str:
    """Send images + text to LLM (Vision API)."""


Auto-select Gemini if available; fallback to OpenAI Vision.

Update .env.example with VISION_PROVIDER=gemini and VISION_API_KEY=.

3Ô∏è‚É£ Integrate with AI-Healer

Modify core/ai_healer.py ‚Üí optional visual fallback:

if healing_failed:
    visual_diffs = vision_analyzer.detect_visual_anomalies("baseline.png", "current.png")
    if visual_diffs:
        healed_locator = vision_analyzer.suggest_locator_from_visuals(visual_diffs)


Tag these logs with healing_source: "vision".

4Ô∏è‚É£ New Tests ‚Üí tests/test_visual_validation.py

Create Playwright + Selenium visual tests:

def test_visual_diff_detection(playwright_page):
    baseline = "tests/screens/baseline_login.png"
    current  = "tests/screens/current_login.png"
    diff     = vision_analyzer.compare_images(baseline, current)
    assert diff["similarity"] < 0.95


Also add:

test_llm_visual_analysis() ‚Äì mock Vision LLM response

test_visual_fallback_healing() ‚Äì simulate visual recovery

Expected: 5-6 tests, all pass.

5Ô∏è‚É£ Docs ‚Üí docs/VISION_TESTING_MODULE.md

Include:

Architecture diagram

VisionAnalyzer class API

Sample LLM prompt for visual diff

Example output (JSON + screenshot annotations)

Performance considerations (caching, rate limits)

üì¶ Deliverables

core/vision_analyzer.py

Updated ai_gateway.py and ai_healer.py

tests/test_visual_validation.py

docs/VISION_TESTING_MODULE.md

Optional sample screenshots in tests/screens/

üß™ Validation Checklist
pytest -s -v tests/test_visual_validation.py


Expected output:

‚úÖ Visual diff detected (0.82 similarity)
‚úÖ AI Vision analysis returned description
‚úÖ Visual fallback healing worked
‚úÖ Logs include "healing_source":"vision"


Check logs for:

{
  "timestamp": "...",
  "healing_source": "vision",
  "region": "button area",
  "description": "Submit button moved slightly left",
  "confidence": 0.93
}

‚ö†Ô∏è Notes

Use existing logging structure.

Do not block tests if no Vision API key (found ‚Üí skip gracefully).

Keep API calls minimal (cached analysis).

Commit with message:

feat(vision): add VisionAnalyzer and visual validation tests

üßæ When Ram Finishes ‚Üí Report Back Format
‚úÖ VisionAnalyzer created
‚úÖ ai_gateway + ai_healer integrated
‚úÖ visual validation tests passing (5/5)
‚ö†Ô∏è Gemini API latency ~2.3s (avg)
Next phase suggestion: Add screenshot diff UI report (Streamlit)
====================================================================
ai_test_foundation ‚Äî Phase 5: Vision Dashboard & Visual Diff UI Reports

Module: Vision Dashboard (UI Reporting)
Owner: Ram
Mentor: Yogi (via ChatGPT)
Repository: ai_test_foundation

üéØ Objective

Create an interactive Streamlit-based web dashboard that:

Displays baseline vs current screenshots side-by-side

Overlays detected diff regions

Shows Vision LLM analysis (text + metrics)

Loads history from logs/vision_cache.json and logs/healing_log.json

Exports HTML/PDF reports for audits

üß† Background Context

Phase 4 gave us:

core/vision_analyzer.py ‚Üí diff + LLM analysis

Cached diff maps + metadata in logs/vision_cache.json

Logs in logs/healing_log.json with healing_source: "vision"

Phase 5 builds a front-end visualization layer, not new AI logic.

üß∞ Work to Do
1Ô∏è‚É£ Create ui/vision_dashboard.py

Implement a Streamlit app with:

import streamlit as st
import json, os
from core.vision_analyzer import VisionAnalyzer
from PIL import Image, ImageDraw

st.set_page_config(page_title="Vision Dashboard", layout="wide")

# Load caches
cache_path = "logs/vision_cache.json"
healing_log = "logs/healing_log.json"


‚úÖ Features to implement step-by-step

Feature	Description
File Upload + Comparison	Upload baseline/current images ‚Üí call VisionAnalyzer.compare_images() ‚Üí show diff metrics & image preview side-by-side
Diff Overlay	Draw bounding boxes from regions in the diff data on top of images
Cached Runs Viewer	List past analyses from vision_cache.json ‚Üí click to reopen
Healing Log Explorer	Display entries with healing_source="vision" ‚Üí sortable table (timestamp, locator, confidence, latency)
LLM Insights Panel	Show Vision LLM description + suggested locator
Export Report	Generate HTML/PDF summary (streamlit-pdf-export or pdfkit)
Dashboard Metrics	Cache hit rate, average similarity, vision usage percentage
2Ô∏è‚É£ Create ui/utils/report_exporter.py

Utility for PDF/HTML generation.

def export_visual_report(diff_data: dict, llm_data: dict, output_path="reports/vision_report.html"):
    """Generate exportable report from diff + analysis."""


‚úÖ Include:

Baseline/current/diff thumbnails (base64)

Change summary table

LLM narrative section

Optional PDF conversion

3Ô∏è‚É£ Docs ‚Üí docs/VISION_DASHBOARD_GUIDE.md

Sections to include:

Dashboard architecture diagram

Key screenshots

Usage workflow (for QA teams)

Report export how-to

Performance considerations

4Ô∏è‚É£ Tests ‚Üí tests/test_vision_dashboard_ui.py

Lightweight integration tests using pytest-streamlit or mocking:

test_load_cached_entries() ‚Äì dashboard reads cache

test_render_diff_overlay() ‚Äì overlay image generated

test_report_exporter_html() ‚Äì HTML file created

üì¶ Deliverables
File	Purpose
ui/vision_dashboard.py	Interactive dashboard (front end)
ui/utils/report_exporter.py	Report generation module
docs/VISION_DASHBOARD_GUIDE.md	User documentation
tests/test_vision_dashboard_ui.py	UI automation tests
requirements.txt update	Add streamlit, pdfkit, reportlab
üß™ Validation Checklist
streamlit run ui/vision_dashboard.py


‚úÖ Expected UI Sections:

Image Comparison Panel

Diff Overlay with bounding boxes

LLM Insights Box

Vision Cache History Sidebar

Export Report Button

Metrics Bar (cache hit rate | avg similarity | vision usage)

‚úÖ Run Tests:

pytest -s -v tests/test_vision_dashboard_ui.py


Expected: 3/3 PASS (HTML report generated, cache loaded, diff overlay rendered)

‚ö†Ô∏è Notes

Do not modify core/vision_analyzer.py.

Handle missing cache files gracefully (empty state UI).

Auto-create /reports/ if missing.

Keep UI theme consistent with Streamlit defaults.

Commit message:

feat(ui): add Vision Dashboard for visual diff reporting
====================================================================
PHASE 6 ‚Äî DOCKER DEPLOYMENT & VERIFICATION

Module: Dockerized AI TestOps Environment
Owner: Ram
Mentor: Yogi (via ChatGPT)
Repository: ai_test_foundation
Goal: Build, run, and verify all 3 containers working in harmony.

üéØ Objective

Deploy the entire ai_test_foundation stack using Docker and Docker Compose, verify that:

All services start successfully (API, Dashboard, Tests)

Containers can communicate via Docker network

Shared volumes (logs/, reports/) synchronize correctly

No dependency issues (Pillow, Playwright, wkhtmltopdf)

Performance and caching behave same as native run

üß† Background

By Phase 5 we finished:

AI-Healer optimization

Vision LLM testing

Streamlit dashboard
Now we encapsulate all of it into reproducible containers.

üß∞ Work to Do
1Ô∏è‚É£ Build the Containers
docker-compose build


Expected result:

Successfully built ai_test_foundation
Successfully tagged ai_test_foundation:latest

2Ô∏è‚É£ Run the API Container
docker-compose up -d api

Verify:

Container: ai_locator_api

Port 8000 active ‚Üí visit http://localhost:8000/docs

Endpoint /health returns JSON:

{ "status": "healthy", "service": "locator-repair-service", "version": "1.0.0" }

3Ô∏è‚É£ Run the Dashboard Container
docker-compose up -d dashboard


Verify:

Container: ai_vision_dashboard

Port 8501 active ‚Üí visit http://localhost:8501

UI loads 5 tabs (Image Diff, LLM Analysis, Cache Viewer, Metrics, Export)

Logs folder mounted (new diffs appear live)

4Ô∏è‚É£ Run the Test Runner
docker-compose run --rm tests


Expected output:

collected 60 items
================== 60 passed in 15.3s ==================


Confirm that logs in /logs/ are updated during run.

5Ô∏è‚É£ Cross-Container Verification

Run inside test container shell:

curl http://api:8000/health


Expected response:
{"status":"healthy"}
‚úÖ proves Docker network connectivity.

Also open browser ‚Üí http://localhost:8501
 ‚Üí Dashboard auto-reads from shared /logs/.

6Ô∏è‚É£ Volume and Cache Verification

Check host logs directory:

ls logs/


Should contain:

healing_log.json

vision_cache.json

Open either:

cat logs/healing_log.json | tail -n 5


Look for new "healing_source":"vision" entries from inside containers.

7Ô∏è‚É£ Performance Check

Run:

docker stats


Confirm:

API container ~150 MB RAM

Dashboard ~200 MB RAM

Tests exit cleanly after run

Total under 500 MB ‚Üí excellent.

8Ô∏è‚É£ Clean Down

When finished:

docker-compose down -v


Removes all containers, networks, volumes cleanly.

üìä Expected Final Output
Verification Item	Expected Result
Build	‚úÖ Successful
API Container	‚úÖ Running, 8000 healthy
Dashboard	‚úÖ Live at 8501
Tests	‚úÖ 100 % pass
Log Sharing	‚úÖ Confirmed
Cross-Network	‚úÖ Confirmed
Volume Sync	‚úÖ Confirmed
Memory Usage	‚úÖ ‚â§ 500 MB
Commit	‚úÖ Docker verified, tag v6.0
‚ö†Ô∏è Notes

Mount your .env file at runtime:

docker run --env-file .env ai_test_foundation


If wkhtmltopdf slow on Windows ‚Üí use HTML export.

Never bake secrets inside images.

To rebuild after code changes:

docker-compose build --no-cache
====================================================================
üß© 1Ô∏è‚É£ Quick Technical Recon ‚Äî cargainqa.rategain.com/#/Login

Front-end Stack: Angular / React hybrid (Webpack bundle)
Automation Difficulty: Moderate to High
DOM Type: Dynamic + Shadow DOM elements (React component IDs change)
Good Targets for AI-Healer:

Login fields

Password input

Login button

Error messages / validation labels

‚úÖ Best Use Case: Self-healing for dynamic locators ‚Äî exactly what AI-Healer and SmartLocator were built for.

‚öôÔ∏è 2Ô∏è‚É£ Page Structure Overview

After loading the page, you‚Äôll see (based on recent inspection):

Element	Typical Locator	Behavior
Username	input[formcontrolname='username'] or #username	Stable but may change in builds
Password	input[formcontrolname='password']	ID/class may change
Login Button	button[type='submit'] or dynamic ID	React-generated, changes with build
Error Toast	.toast-message	Appears dynamically
Remember Me	Checkbox with dynamic label binding	Label text only

This site uses Angular dynamic rendering, so Playwright/Selenium locators will occasionally fail ‚Äî ideal for testing auto-healing and AI-based recovery.

üß† 3Ô∏è‚É£ Test Objective

We‚Äôre validating:

AI-Healer auto-repair when locators break.

SmartLocator‚Äôs context-based re-identification.

VisionAnalyzer screenshot comparison after login failure.

Logging + Dashboard integration (logs, vision_cache).

Dockerized test execution on the same target.

üß™ 4Ô∏è‚É£ Real-World Test Cases ‚Äî Cargain Login
Test Case #1 ‚Äî Valid Login (Baseline)
Step	Action	Expected
1	Open https://cargainqa.rategain.com/#/Login	Page loads fully
2	Locate Username field	Locator found by SmartLocator
3	Enter valid username	Field accepts input
4	Enter valid password	Field accepts input
5	Click ‚ÄúLogin‚Äù	Dashboard/redirect occurs
6	Capture baseline screenshot	Stored for future Vision tests
7	Verify ‚ÄúLogin Successful‚Äù or page redirect	Success logged

‚úÖ Confirms your framework can run without any healing trigger.

Test Case #2 ‚Äî Locator Broken ‚Üí AI-Healer Recovery
Step	Action	Expected
1	Replace locator in test to a fake one (e.g. #username-broken)	Locator fails
2	Run test with AI-Healer	Auto-healing triggers
3	Logs show "healing_source": "ai"	Confirmed in healing_log.json
4	New locator found (input[placeholder='Username'])	Success
5	Field filled successfully	Test continues
6	Healing latency logged (<500 ms avg)	Verified

‚úÖ This is your self-healing test validation ‚Äî essential proof that SmartLocator + AI-Healer work.

Test Case #3 ‚Äî Vision Validation on Login Button
Step	Action	Expected
1	Capture baseline screenshot (baseline_login.png)	Stored
2	Trigger small visual change (simulate UI shift or dark theme)	New screenshot
3	Run VisionAnalyzer.compare_images()	Detects 5‚Äì10% diff
4	Vision LLM explains: ‚ÄúLogin button color changed‚Äù	‚úÖ
5	Dashboard shows diff map	‚úÖ

‚úÖ Confirms VisionAnalyzer and Dashboard integration.

Test Case #4 ‚Äî Incorrect Credentials Handling
Step	Action	Expected
1	Enter invalid username/password	Error toast appears
2	Capture screenshot	Stored
3	VisionAnalyzer detects new element (toast)	‚úÖ
4	Vision LLM describes: ‚ÄúError message appeared on top‚Äù	‚úÖ
5	Dashboard metrics update (new anomaly logged)	‚úÖ

‚úÖ Confirms Vision fallback and anomaly detection on dynamic UI elements.

Test Case #5 ‚Äî Regression Check via Docker
Step	Action	Expected
1	Run test suite via Docker	docker-compose run tests pytest -s -v tests/test_cargain_login.py
2	Verify logs update (healing + vision)	‚úÖ
3	Open Dashboard ‚Üí Healing Logs	Shows entries for AI/vision healing
4	All tests pass (5/5)	‚úÖ
5	Memory < 500 MB	‚úÖ

‚úÖ Confirms your Dockerized project works end-to-end with a live app.

üß∞ 5Ô∏è‚É£ Implementation Example ‚Äì Playwright + AI-Healer Integration

Save as tests/test_cargain_login.py:

import pytest
from core.smart_locator.smart_locator import SmartLocator
from core.ai_healer import AIHealer
from playwright.sync_api import sync_playwright

@pytest.mark.parametrize("username,password", [("yourUser", "yourPass")])
def test_cargain_login_healing(username, password):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        page.goto("https://cargainqa.rategain.com/#/Login")

        healer = AIHealer(enable_vision=True)
        locator = SmartLocator(page)

        # Intentionally broken locator
        broken_locator = "#username-broken"

        # AI-Healer attempts repair
        healed = healer.heal_locator(
            page,
            failed_locator=broken_locator,
            context_hint="Username input field",
            engine="Playwright"
        )
        assert healed is not None, "Healing failed for Username"

        page.fill(healed, username)

        # Now login button
        login_locator = "#login-btn-missing"
        healed_button = healer.heal_locator(page, login_locator, "Login button", "Playwright")
        page.click(healed_button)

        browser.close()


‚úÖ This test will:

Trigger AI-Healer for username & login button

Auto-repair missing locators

Log the healing in logs/healing_log.json

Optionally trigger Vision fallback if locators visually differ

üìä 6Ô∏è‚É£ Expected Logs

logs/healing_log.json will show:

{
  "timestamp": "2025-11-12T14:05:33.456",
  "engine": "Playwright",
  "old_locator": "#username-broken",
  "new_locator": "input[placeholder='Username']",
  "healing_source": "ai",
  "latency_ms": 327.2,
  "confidence": 0.96
}


and VisionAnalyzer logs in logs/vision_cache.json:

{
  "description": "Login button moved slightly lower",
  "diff_percentage": 6.8,
  "confidence": 0.89
}

‚úÖ 7Ô∏è‚É£ Run & Verify

Inside your Docker setup:

docker-compose up api dashboard
docker-compose run tests pytest -s -v tests/test_cargain_login.py


Then check:

healing_log.json for AI/vision entries

vision_cache.json for diff reports

Dashboard (http://localhost:8501
) ‚Üí Healing Logs tab for entries

üî• 8Ô∏è‚É£ If Everything Works

Then you‚Äôll have confirmed proof that your AI-Healer + SmartLocator + VisionAnalyzer stack heals and analyzes a real-world React/Angular login page.
=======================================================================
Developer Prompt ‚Äî Phase 7: Universal AI Interaction Layer (AI-Interactor + Proxy Handler)

Module: Universal AI Interaction Layer (aka AI-Interactor + Proxy Handler + Adapter Hardenings)
Owner: Ram
Mentor: Yogi (via ChatGPT)
Repo: ai_test_foundation (work inside this repo branch: feature/universal-interactor)

üéØ Objective (short & brutal)

Make the framework actually test any website ‚Äî including anti-automation protected SPAs ‚Äî by adding an adaptive interaction layer, stealth tooling, and proxy support. Integrate it with existing SmartLocator + AIHealer so healed locators can be actually used even when .fill() or .click() are blocked.

No fluff: if a site blocks conventional automation, framework must detect that and switch to safe fallbacks (JS injection, simulated human actions, proxy header scrubbing, or HTTP adapters). Tests must prove this works on https://cargainqa.rategain.com/#/Login and at least one other site (SauceDemo or The-Internet).

‚úÖ Deliverables (what you must produce)

core/ai_interactor.py ‚Äî new adaptive interaction engine (production-grade, typed).

services/proxy_handler.py ‚Äî local proxy manager (mitmproxy integration or instructions).

adapters/playwright_adapter.py ‚Äî hardened Playwright adapter using stealth + safe actions.

Minor updates in core/ai_healer.py to call AIInteractor.safe_* methods instead of direct .fill() / .click().

Fix vision tests param name issue (baseline_image ‚Üí baseline_path).

Tests:

tests/universal/test_cargain_universal.py ‚Äî checks healing + safe interactions on Cargain login.

tests/universal/test_saucedemo_universal.py ‚Äî backup target for visual tests.

docs/UNIVERSAL_INTERACTOR.md ‚Äî design, fallback hierarchy, how to run with/without proxy.

Short verification report file docs/phase7_verification.md to be completed after run.

‚öôÔ∏è Key design rules (non-negotiable)

Do not hardcode credentials or API keys in images. Use .env mounted at runtime.

Keep all new code well-typed and unit-tested.

Fail gracefully: if proxy not available, run reduced (no-stealth) mode and log interactor_mode: degraded.

Always log interaction_method in healing logs: values = ["direct", "js_inject", "human_typing", "http_adapter", "degraded"].

Keep original method signatures of public API; add optional args/flags only.

üìÇ New file blueprints & code snippets
1) core/ai_interactor.py (skeleton)

Create the file and implement robust fallbacks:

# core/ai_interactor.py
from typing import Optional
import time
import logging
from playwright.sync_api import Page, TimeoutError as PWTimeoutError

log = logging.getLogger(__name__)

class AIInteractor:
    """
    Adaptive interactor that attempts actions in order:
      1. direct framework action (page.fill / page.click)
      2. JS injection (set value + dispatch input events)
      3. simulated human typing (character-by-character)
      4. HTTP adapter (submit via REST if available) - optional
    """

    def __init__(self, page: Page, timeout: float = 5000):
        self.page = page
        self.timeout = timeout / 1000.0

    def safe_fill(self, selector: str, value: str) -> bool:
        start = time.perf_counter()
        # 1) Try native fill
        try:
            self.page.fill(selector, value, timeout=int(self.timeout * 1000))
            log.info("[AIInteractor] fill via native succeeded")
            self._log_method('direct', start)
            return True
        except Exception as e:
            log.warning("[AIInteractor] native fill failed: %s", e)

        # 2) Try JS injection
        try:
            self.page.evaluate(
                """(sel, val) => {
                    const el = document.querySelector(sel);
                    if (!el) return false;
                    el.focus();
                    el.value = val;
                    el.dispatchEvent(new Event('input', { bubbles: true }));
                    el.dispatchEvent(new Event('change', { bubbles: true }));
                    return true;
                }""",
                selector,
                value,
            )
            log.info("[AIInteractor] fill via JS injection succeeded")
            self._log_method('js_inject', start)
            return True
        except Exception as e:
            log.warning("[AIInteractor] js injection failed: %s", e)

        # 3) Human-like typing - fallback slow but effective
        try:
            for ch in value:
                self.page.type(selector, ch, delay=50)  # 50ms per char
            log.info("[AIInteractor] fill via human typing succeeded")
            self._log_method('human_typing', start)
            return True
        except Exception as e:
            log.warning("[AIInteractor] human typing failed: %s", e)

        # 4) final degrade
        self._log_method('degraded', start)
        return False

    def safe_click(self, selector: str) -> bool:
        # Similar pattern: try click -> js dispatch click -> focus+enter -> degrade
        ...
    
    def _log_method(self, method: str, start_time: float):
        elapsed = (time.perf_counter() - start_time) * 1000.0
        log.info("[AIInteractor] method=%s elapsed_ms=%.2f", method, elapsed)
        # append to healing_log via existing mechanism if needed


Implement safe_click the same way (native .click(), then JS el.click(), then send Enter key).

2) adapters/playwright_adapter.py (hardening)

Wrap Playwright launching with stealth measures and env knobs.

Use Playwright options: args=["--disable-blink-features=AutomationControlled"] and set navigator.webdriver via eval_on_new_document.

Consider adding headers, user-agent rotation (basic) and optional proxy settings.

Snippet:

# adapters/playwright_adapter.py
from playwright.sync_api import sync_playwright

def launch_stealth_browser(headless=True, proxy: Optional[str] = None):
    pw = sync_playwright().start()
    browser = pw.chromium.launch(
        headless=headless,
        args=[
            "--disable-blink-features=AutomationControlled",
            "--no-sandbox",
            "--disable-dev-shm-usage",
        ],
        proxy={"server": proxy} if proxy else None
    )
    context = browser.new_context()
    # stealth: modify navigator.webdriver and other properties
    context.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
        window.navigator.chrome = { runtime: {} };
        Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
    """)
    return pw, browser, context


Keep this as a helper used by tests or demo scripts ‚Äî do NOT make it mandatory for all runs (configurable via .env).

3) services/proxy_handler.py (mitmproxy or simple header scrubber)

If you want deeper stealth (mitmproxy), provide launching helper and config. Minimal version: start/stop mitmproxy in Docker or instruct on external startup.

Design:

Provide start_proxy() / stop_proxy() helpers that optionally spawn mitmproxy subprocess with rules to rewrite headers and strip suspicious automation headers.

Add env var USE_PROXY=true and PROXY_URL=http://proxy:8080.

Important: For enterprise use, mitmproxy requires cert handling; document this in docs/UNIVERSAL_INTERACTOR.md and provide --insecure flags for test-only environments.

üîÅ Integration points ‚Äî modify AI-Healer to use AIInteractor

In core/ai_healer.py replace direct page.fill() / page.click() usages with calls to AIInteractor.safe_fill() and AIInteractor.safe_click() when performing healing actions. Keep API signature compatibility; simply construct AIInteractor(page) when needed.

Also update healing logging to include interaction_method.

üß™ Tests to add (must be robust)

tests/universal/test_cargain_universal.py ‚Äî end-to-end scenario:

Launch Playwright in stealth mode (use adapters.playwright_adapter.launch_stealth_browser).

Start API & Dashboard containers (or assume running in Docker compose test run).

Run:

Load login page

Break locator (use deliberate broken selector)

Call AIHealer.heal_locator(...)

After healed locator returned, call AIInteractor.safe_fill() to input username/password (assert it returns True)

Attempt login via safe_click and assert redirect or success indicator

On failure due to site stricter blocks, assert that safe_fill() fell back to js_inject or human_typing (read logs).

tests/universal/test_saucedemo_universal.py ‚Äî same but easier site; must pass.

Fix parameter bug: update any tests where baseline_image was used ‚Äî rename to baseline_path.

Unit tests for AIInteractor methods (mock page object with exceptions to simulate failing paths).

üßæ Run & Verify (exact commands)

Branch:

git checkout -b feature/universal-interactor


Run tests locally (fast mode):

# install new deps
pip install mitmproxy playwright
playwright install chromium

# unit tests
pytest -q tests/unit/test_ai_interactor.py


Run integration (Dockerized)

# Ensure docker-compose has services api/dashboard/tests
docker-compose up -d api dashboard
# run test container
docker-compose run --rm tests pytest -q tests/universal/test_cargain_universal.py::test_cargain_login_universal -s


Expected results:

AI-Healer returns healed locators (cache or ai)

AIInteractor.safe_fill() returns True and logs interaction_method (either "direct", "js_inject", or "human_typing")

Healing logs updated with interaction_method and interaction_latency_ms

If proxy used, confirm proxy_handler logs show active proxy injection

üßØ Edge Cases & Defensive Notes (be critical)

Playwright stealth will fail against advanced detection (bot-detection services). We must not claim impossible: if site uses server-side verification (CAPTCHA, fingerprinting), we can‚Äôt beat it automatically ‚Äî we can only detect it and report blocked_by_site.

JS injection may cause events to fire differently; ensure triggering both input and change events, and if necessary blur() / focus().

Human-typing delays should be randomized slightly (45‚Äì80ms) to look natural ‚Äî but keep tests deterministic enough by seeding delays for tests.

Mitmproxy needs cert trust for HTTPS interception ‚Äî for testing, use --insecure options or configure browser context to ignore cert errors (context = browser.new_context(ignore_https_errors=True)).

ALWAYS log blocked_by_site: true/false when detection of automation protection occurs (timeouts, blocked requests, unusual responses).

üßæ Documentation & Reporting

Add docs/UNIVERSAL_INTERACTOR.md covering:

Goals & architecture

How to run with/without proxy

How AIInteractor chooses fallbacks

Known limitations and how to interpret blocked_by_site logs

Create docs/phase7_verification.md template for Ram to fill:

Commit hash

Tests run & results

Interaction method counts

Any failures with raw logs

Recommendation for next step (analytics/packaging)

üì¶ Extra (bonus improvements ‚Äî not mandatory now)

Add interaction_mode config: aggressive (try HTTP adapter), stealth (use proxy + stealth), safe (no proxy).

Add adapters/http_adapter.py for apps where DOM impossible but REST endpoints exist: e.g., for login you may POST to /api/login if discovered.

Add diagnostics endpoint in locator_repair/api.py to return last blocked_by_site events, counts, and top failed selectors.

‚úÖ Acceptance Criteria (what I‚Äôll check)

When Ram reports back, I need a short clear report:

‚úÖ Branch: feature/universal-interactor
‚úÖ Files added/modified: core/ai_interactor.py, adapters/playwright_adapter.py, services/proxy_handler.py, core/ai_healer.py
‚úÖ Unit tests: tests/unit/test_ai_interactor.py - 6/6 passed
‚úÖ Integration tests: tests/universal/test_cargain_universal.py - 1/1 passed
‚úÖ Interaction methods observed: {"direct":2,"js_inject":1,"human_typing":0}
‚úÖ Logs: healing_log.json includes "interaction_method" and "blocked_by_site" flags
‚úÖ Proxy: optional start/stop script provided and tested (mitmproxy)
Notes: [any blockers / suggestions]
